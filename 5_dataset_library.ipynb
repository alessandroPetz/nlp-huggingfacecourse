{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-11 12:12:28--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n",
      "--2025-03-11 12:12:28--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7725286 (7.4M) [application/octet-stream]\n",
      "Saving to: ‘./dataset/SQuAD_it-train.json.gz.1’\n",
      "\n",
      "SQuAD_it-train.json 100%[===================>]   7.37M  10.9MB/s    in 0.7s    \n",
      "\n",
      "2025-03-11 12:12:29 (10.9 MB/s) - ‘./dataset/SQuAD_it-train.json.gz.1’ saved [7725286/7725286]\n",
      "\n",
      "--2025-03-11 12:12:29--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n",
      "--2025-03-11 12:12:29--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051245 (1.0M) [application/octet-stream]\n",
      "Saving to: ‘./dataset/SQuAD_it-test.json.gz.1’\n",
      "\n",
      "SQuAD_it-test.json. 100%[===================>]   1.00M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-03-11 12:12:29 (10.6 MB/s) - ‘./dataset/SQuAD_it-test.json.gz.1’ saved [1051245/1051245]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 442 examples [00:00, 454.68 examples/s]\n",
      "Generating test split: 48 examples [00:00, 611.74 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fatto ciò provare a usare i dataset delle batterie\n",
    "\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz -P ./dataset/\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz -P ./dataset/\n",
    "\n",
    "\n",
    "# carico il dataset in json unendo training e validation\n",
    "# la funzione load automaticamente decomprime da .gz\n",
    "data_files = {\"train\": \"dataset/SQuAD_it-train.json.gz\", \"test\": \"dataset/SQuAD_it-test.json.gz\"} \n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2726083861.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    url = \"https://github.com/crux82/squad-it/raw/master/\" -P ./dataset/\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# direttamente da cartella remota\n",
    "\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\" \n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLICE and DICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apetrella/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-12 09:52:56--  https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘./dataset/drugsCom_raw.zip.3’\n",
      "\n",
      "drugsCom_raw.zip.3      [        <=>         ]  41.00M  8.97MB/s    in 5.5s    \n",
      "\n",
      "2025-03-12 09:53:02 (7.44 MB/s) - ‘./dataset/drugsCom_raw.zip.3’ saved [42989872]\n",
      "\n",
      "Archive:  dataset/drugsCom_raw.zip\n",
      "caution: filename not matched:  -P\n",
      "caution: filename not matched:  ./dataset/\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\" -P ./dataset/\n",
    "!unzip \"dataset/drugsCom_raw.zip\" -P ./dataset/\n",
    "data_files = {\"train\": \"dataset/drugsComTrain_raw.tsv\", \"test\": \"dataset/drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 problemi:\n",
    " 1. unnamed 0 cos'è?\n",
    " 2. condition labels mix tra uppercase e lowercase\n",
    " 3. review pieno di refusi tra caratteri pyhton 3 html\n",
    "\n",
    "######   SOluzioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "# questo unnamed 0 cosa vuole dire?, controlliamo che sia un id unico per paziente\n",
    "\n",
    "\n",
    "\n",
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))\n",
    "\n",
    "# hanno la stessa lunghezza, questo significa che è un campo univoco, diciamo che è l'id\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160398/160398 [00:06<00:00, 25340.86 examples/s]\n",
      "Map: 100%|██████████| 53471/53471 [00:02<00:00, 26455.83 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['left ventricular dysfunction', 'adhd', 'birth control']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.\n",
    "#normalizziamo le condition labels, tutte in minuscolo, utilizzando .map\n",
    "\n",
    "#def lowercase_condition(example):\n",
    "#    return {\"condition\": example[\"condition\"].lower()}\n",
    "#  drug_dataset.map(lowercase_condition)\n",
    "\n",
    "# ci sono dei noneType, quindi dobbiamo filtrarli\n",
    "\n",
    "# def filter_nones(x):\n",
    "#     return x[\"condition\"] is not None   # ritorna true (!= da None) o false  \n",
    "\n",
    "# drug_dataset.filter(filter_nones)   # filter è molto simile a .map (true lo tiene, false lo scarta)\n",
    "\n",
    "# possiamo farlo utillizando una lambda function (lambda <arguments> : <expression>)\n",
    "# es (lambda x: x * x)(3) = 9\n",
    "# es (lambda base, height: 0.5 * base * height)(4, 8) = 16\n",
    "\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)     # uguale a filter_nones!!!!!\n",
    "# tolte i None, normalizziamo a lowercase\n",
    "drug_dataset = drug_dataset.map(lambda x : {\"condition\": x[\"condition\"].lower()} ) # uguale a lowercase_condition\n",
    "# Check that lowercasing worked\n",
    "drug_dataset[\"train\"][\"condition\"][:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160398/160398 [00:05<00:00, 29937.21 examples/s]\n",
      "Map: 100%|██████████| 53471/53471 [00:01<00:00, 30459.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 160398, 'test': 53471}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 160398/160398 [00:00<00:00, 343835.75 examples/s]\n",
      "Filter: 100%|██████████| 53471/53471 [00:00<00:00, 343002.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#2.5\n",
    "\n",
    "# creiamo una colonna con la lunghezza delle review (in numero di parole)\n",
    "\n",
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "drug_dataset = drug_dataset.map(compute_review_length)        #.map chiama la funzione su ogni elemento e aggiunge l'elemento(perchè non lo trova, altrimenti lo modificherebbe)\n",
    "\n",
    "print(drug_dataset.num_rows)\n",
    "# eliminiamo i campi con le review < 30\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the Dataset.sort() function to inspect the reviews with the largest numbers of words. See the documentation to see which argument you need to use sort the reviews by length in descending order.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdrug_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/dataset_dict.py:1140\u001b[0m, in \u001b[0;36mDatasetDict.sort\u001b[0;34m(self, column_names, reverse, null_placement, keep_in_memory, load_from_cache_file, indices_cache_file_names, writer_batch_size)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices_cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1138\u001b[0m     indices_cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m-> 1140\u001b[0m     {\n\u001b[1;32m   1141\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39msort(\n\u001b[1;32m   1142\u001b[0m             column_names\u001b[38;5;241m=\u001b[39mcolumn_names,\n\u001b[1;32m   1143\u001b[0m             reverse\u001b[38;5;241m=\u001b[39mreverse,\n\u001b[1;32m   1144\u001b[0m             null_placement\u001b[38;5;241m=\u001b[39mnull_placement,\n\u001b[1;32m   1145\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m   1146\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m   1147\u001b[0m             indices_cache_file_name\u001b[38;5;241m=\u001b[39mindices_cache_file_names[k],\n\u001b[1;32m   1148\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m   1149\u001b[0m         )\n\u001b[1;32m   1150\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1151\u001b[0m     }\n\u001b[1;32m   1152\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/dataset_dict.py:1141\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices_cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1138\u001b[0m     indices_cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m   1140\u001b[0m     {\n\u001b[0;32m-> 1141\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnull_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_placement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindices_cache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_cache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1151\u001b[0m     }\n\u001b[1;32m   1152\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/arrow_dataset.py:4308\u001b[0m, in \u001b[0;36mDataset.sort\u001b[0;34m(self, column_names, reverse, null_placement, keep_in_memory, load_from_cache_file, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[1;32m   4298\u001b[0m sort_table \u001b[38;5;241m=\u001b[39m query_table(\n\u001b[1;32m   4299\u001b[0m     table\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data,\n\u001b[1;32m   4300\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)),\n\u001b[1;32m   4301\u001b[0m     indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices,\n\u001b[1;32m   4302\u001b[0m )\n\u001b[1;32m   4304\u001b[0m sort_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   4305\u001b[0m     (col, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascending\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m col_reverse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescending\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col, col_reverse \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(column_names, reverse)\n\u001b[1;32m   4306\u001b[0m ]\n\u001b[0;32m-> 4308\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43msort_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnull_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m   4311\u001b[0m     indices\u001b[38;5;241m=\u001b[39mindices,\n\u001b[1;32m   4312\u001b[0m     keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4315\u001b[0m     new_fingerprint\u001b[38;5;241m=\u001b[39mnew_fingerprint,\n\u001b[1;32m   4316\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/pyarrow/compute.py:263\u001b[0m, in \u001b[0;36m_make_generic_wrapper.<locals>.wrapper\u001b[0;34m(memory_pool, options, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Expression):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Expression\u001b[38;5;241m.\u001b[39m_call(func_name, \u001b[38;5;28mlist\u001b[39m(args), options)\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the Dataset.sort() function to inspect the reviews with the largest numbers of words. See the documentation to see which argument you need to use sort the reviews by length in descending order.\n",
    "# print(drug_dataset.sort(\"review\", reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 161297/161297 [00:00<00:00, 867050.96 examples/s] \n",
      "Map (num_proc=2): 100%|██████████| 53766/53766 [00:00<00:00, 471414.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 ms, sys: 35.2 ms, total: 63.3 ms\n",
      "Wall time: 346 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. \n",
    "# sistemare caratteri html\n",
    "\n",
    "import html\n",
    "%time drug_dataset = drug_dataset.map(       lambda x: {\"review\": html.unescape(x[\"review\"])},  batched=True, num_proc = 2   )    # per ogni elemento, lo aggiorna con la nuova review non-html\n",
    "\n",
    "# batched=True  serve a mandare un batch alla volta alla funzione, e non un elemento alla volta, per velocizzare!\n",
    "# num_proc= 8   serve a parellelizzare \n",
    "\n",
    "\n",
    "# quindi al posto di loopare su ogni elemento, utilizza la list comprehension(?) su ogni batch\n",
    "# NB questa cosa viene utilizzata anche per il TOKENIZATION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 53766/53766 [00:14<00:00, 3635.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': ['input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask',\n",
       "  'overflow_to_sample_mapping'],\n",
       " 'test': ['input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask',\n",
       "  'overflow_to_sample_mapping']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### return_overflowing_tokens=True\n",
    "# serve a troncare a tot, ma tenere anche il resto dell'input(troncandolo eventualmente)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=9,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "# result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "# [len(inp) for inp in result[\"input_ids\"]]\n",
    "\n",
    "# per utilizzare la funzione dobbiamo rimuovere le colonne derivate dal vecchio dataset = drug_dataset[\"train\"].column_names.\n",
    "# questo perchè altrimenti il dataset di partenza e finale hanno lunghezze diverse e a .map non piace\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names)\n",
    "len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])\n",
    "\n",
    "tokenized_dataset.column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drug_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#usando questo mapping manteniamo le colonne.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# se queste colonne ci servono per un eventuale post-processing, dobbiamo usare questo approccio.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdrug_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(tokenize_and_split, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m tokenized_dataset\u001b[38;5;241m.\u001b[39mcolumn_names\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drug_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# oppure possiamo rendere più lungo il dataset di partenza.\n",
    "\n",
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "#usando questo mapping manteniamo le colonne.\n",
    "# se queste colonne ci servono per un eventuale post-processing, dobbiamo usare questo approccio.\n",
    "\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Dataset s to DataFrame s and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prime 3 righe del dataset\n",
      "{'patient_id': [206461, 95260, 92703], 'drugName': ['Valsartan', 'Guanfacine', 'Lybrel'], 'condition': ['Left Ventricular Dysfunction', 'ADHD', 'Birth Control'], 'review': ['\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"', '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"', '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it\\'s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn\\'t have any other side effects. The idea of being period free was so tempting... Alas.\"'], 'rating': [9.0, 8.0, 5.0], 'date': ['May 20, 2012', 'April 27, 2010', 'December 14, 2009'], 'usefulCount': [27, 192, 17]}\n",
      "\n",
      " prime 3 righe del dataset in formato pandas\n",
      "   patient_id    drugName                     condition  \\\n",
      "0      206461   Valsartan  Left Ventricular Dysfunction   \n",
      "1       95260  Guanfacine                          ADHD   \n",
      "2       92703      Lybrel                 Birth Control   \n",
      "\n",
      "                                              review  rating  \\\n",
      "0  \"It has no side effect, I take it in combinati...     9.0   \n",
      "1  \"My son is halfway through his fourth week of ...     8.0   \n",
      "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
      "\n",
      "                date  usefulCount  \n",
      "0       May 20, 2012           27  \n",
      "1     April 27, 2010          192  \n",
      "2  December 14, 2009           17  \n",
      "\n",
      " head del pandas frequencies\n",
      "       frequency  count\n",
      "0  Birth Control  28788\n",
      "1     Depression   9069\n",
      "2           Pain   6145\n",
      "3        Anxiety   5904\n",
      "4           Acne   5588\n",
      "\n",
      " stampa del dataframe derivato dal pandas frequencies\n",
      "Dataset({\n",
      "    features: ['frequency', 'count'],\n",
      "    num_rows: 884\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "drug_dataset.reset_format()         # resetto il formato\n",
    "\n",
    "print(\"prime 3 righe del dataset\")\n",
    "print(drug_dataset[\"train\"][:3])    # print as a dict\n",
    "\n",
    "print(\"\\n prime 3 righe del dataset in formato pandas\")\n",
    "drug_dataset.set_format(\"pandas\")   # cambio in pandas  (è solo il formato riotrnato di quando viene chiamato, resta un dataset!!!)\n",
    "print(drug_dataset[\"train\"][:3])    # print in pandas version\n",
    "\n",
    "train_df = drug_dataset[\"train\"][:]  # creo un pandas.Dataframe dall'intero train_set (prima era solo visto come un pandas, ora train_df è un pandas)\n",
    "\n",
    "# ora posso fare tutte le operazioni possibili in un pandas\n",
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "\n",
    "print(\"\\n head del pandas frequencies\")\n",
    "print(frequencies.head())\n",
    "\n",
    "from datasets import Dataset\n",
    "freq_dataset = Dataset.from_pandas(frequencies)   # creo un dataset da un pandas\n",
    "print(\"\\n stampa del dataframe derivato dal pandas frequencies\")\n",
    "print(freq_dataset)         \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drug_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m drug_dataset_clean \u001b[38;5;241m=\u001b[39m \u001b[43mdrug_dataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_test_split(train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Rename the default \"test\" split to \"validation\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m drug_dataset_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m drug_dataset_clean\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drug_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a dataset\n",
    "\n",
    "### 3 Methods\n",
    "Data format     Function\n",
    "Arrow           Dataset.save_to_disk()\n",
    "CSV\t            Dataset.to_csv()\n",
    "JSON\t        Dataset.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apetrella/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'drug_dataset_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdrug_dataset_clean\u001b[49m\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/drug-reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# save evrey split in a different folder\u001b[39;00m\n\u001b[1;32m      6\u001b[0m drug_dataset_reloaded \u001b[38;5;241m=\u001b[39m load_from_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/drug-reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load from the main folder\u001b[39;00m\n\u001b[1;32m      7\u001b[0m drug_dataset_reloaded\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drug_dataset_clean' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_clean.save_to_disk(\"dataset/drug-reviews\")   # save evrey split in a different folder\n",
    "\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"dataset/drug-reviews\")  # load from the main folder\n",
    "drug_dataset_reloaded\n",
    "\n",
    "\n",
    "\n",
    "# For the CSV and JSON formats, we have to store each split as a separate file. \n",
    "# One way to do this is by iterating over the keys and values in the DatasetDict object:\n",
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"dataset/drug-reviews-{split}.jsonl\")    #save json\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)   #load json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 128318\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 32080\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 53471\n",
      "    })\n",
      "})\n",
      "colonne rimaste nel dataset:\n",
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "✅ Tutte le condizioni del test set esistono nel training set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46775/46775 [00:05<00:00, 7860.03 examples/s]\n",
      "Map: 100%|██████████| 78232/78232 [00:09<00:00, 7850.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\" -P ./dataset/\n",
    "#!unzip \"dataset/drugsCom_raw.zip\" -P ./dataset/\n",
    "data_files = {\"train\": \"dataset/drugsComTrain_raw.tsv\", \"test\": \"dataset/drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "\n",
    "#rinomino Unnamed con id hanno la stessa lunghezza, questo significa che è un campo univoco, diciamo che è l'id\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "#### condition tutto minuscolo ####\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)     # uguale a filter_nones!!!!!\n",
    "# tolte i None, normalizziamo a lowercase\n",
    "drug_dataset = drug_dataset.map(lambda x : {\"condition\": x[\"condition\"].lower()} ) # uguale a lowercase_condition\n",
    "# Check that lowercasing worked\n",
    "drug_dataset[\"train\"][\"condition\"][:10]\n",
    "\n",
    "### sistem html in drug review\n",
    "import html\n",
    "drug_dataset = drug_dataset.map( lambda x: {\"review\": html.unescape(x[\"review\"])},  batched=True, num_proc = 2   )    # per ogni elemento, lo aggiorna con la nuova review non-html\n",
    "drug_dataset = drug_dataset.map( lambda x: {\"condition\": html.unescape(x[\"condition\"])},  batched=True, num_proc = 2   )\n",
    "### creo validation\n",
    "\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset = drug_dataset_clean\n",
    "\n",
    "print(drug_dataset)\n",
    "\n",
    "## tokenizzo rimuovendo tutte le colonne in più\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "    \n",
    "\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "tokenized_dataset\n",
    "\n",
    "number_of_labels = len(set(tokenized_dataset[\"train\"][\"condition\"]))\n",
    "number_of_labels\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ELIMIARE LE COLONNE (e altro )NEL DATASET CHE NON CI INTERESSANO (La classe trainer lo fa in automatico)\n",
    "\n",
    "tokenized_datasets = tokenized_dataset.remove_columns([\"patient_id\", \"drugName\", \"review\", \"rating\", \"date\", \"usefulCount\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"condition\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(\"colonne rimaste nel dataset:\")\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "# creo una mappatura labels id e poi lo applico. il modello vuole solo numeri \n",
    "unique_conditions = list(set(tokenized_datasets[\"train\"][\"labels\"]))\n",
    "condition_to_id = {cond: idx for idx, cond in enumerate(unique_conditions)}\n",
    "\n",
    "# per fare veloce, rimuovo le condiizoni non presenti nel train test\n",
    "filtered_test_set = tokenized_datasets[\"test\"].filter(lambda x: x[\"labels\"] in condition_to_id)\n",
    "tokenized_datasets[\"test\"] = filtered_test_set\n",
    "\n",
    "# per fare veloce, rimuovo le condiizoni non presenti nel train e validation test\n",
    "filtered_test_set = tokenized_datasets[\"validation\"].filter(lambda x: x[\"labels\"] in condition_to_id)\n",
    "tokenized_datasets[\"validation\"] = filtered_test_set\n",
    "\n",
    "# # Troviamo tutte le condizioni nel test set\n",
    "# test_conditions = set(tokenized_datasets[\"validation\"][\"labels\"])\n",
    "\n",
    "# # Troviamo le condizioni che NON sono nel training set\n",
    "# unknown_conditions = test_conditions - set(condition_to_id.keys())\n",
    "\n",
    "# # Stampiamo eventuali condizioni sconosciute\n",
    "# if unknown_conditions:\n",
    "#     print(\"⚠️ Attenzione! Le seguenti condizioni sono presenti nel test set ma non nel training set:\")\n",
    "#     print(len(unknown_conditions))\n",
    "# else:\n",
    "#     print(\"✅ Tutte le condizioni del test set esistono nel training set.\")\n",
    "\n",
    "def encode_labels(example):\n",
    "    return {\"labels\": condition_to_id[example[\"labels\"]]}\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(encode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/apetrella/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8221, grad_fn=<NllLossBackward0>) torch.Size([8, 838])\n",
      "70515\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 26117/70515 [14:13<24:07, 30.68it/s]"
     ]
    }
   ],
   "source": [
    "# DEFINIAMO IL DATALOADER\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8,collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8,collate_fn=data_collator\n",
    ")\n",
    "for batch in train_dataloader: # CHECK CORRECTENESS OF DATA\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "\n",
    "# INSTANZIO IL MODELLO\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=number_of_labels)\n",
    "outputs = model(**batch)   # CONTROLLLO CHE TUTTO È OK passando il batch (batch da 8, 2 logits)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "\n",
    "\n",
    "            #### CREIAMO IL TRAINING ####\n",
    "\n",
    "# OTTIMIZZATORE E LR SCHEDULER, COME QUELLI DEL TRAINING USATO SOPRA\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)\n",
    "\n",
    "# SCELGO LA gpu COME DEVICE\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "# THE LOOP TRAINING\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()   # SETTA IL MODELLO PER ESSERE TRAINATO\n",
    "for epoch in range(num_epochs):   # TUTTO IL DATALOADER PER TOT EPOCHE\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metriche?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization pipeline\n",
    "summ = pipeline(\"summarization\")\n",
    "for review in tokenized_datasets[\"train\"][\"labels\"]:\n",
    "    print(summ( review ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
